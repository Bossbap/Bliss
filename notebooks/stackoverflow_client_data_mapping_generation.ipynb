{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37637d1f",
   "metadata": {},
   "source": [
    "## Create Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a3e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Data/baptiste.geisenberger/Data/stackoverflow/client_data_mapping/train.csv  (rows=342481)\n",
      "Wrote: /Data/baptiste.geisenberger/Data/stackoverflow/client_data_mapping/test.csv   (rows=204090)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATASET_ROOT = Path(\"/Data/baptiste.geisenberger/Data/stackoverflow\")\n",
    "TRAIN_DIR = DATASET_ROOT / \"train\"\n",
    "TEST_DIR  = DATASET_ROOT / \"test\"\n",
    "\n",
    "OUTPUT_DIR = DATASET_ROOT / \"client_data_mapping\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLUSTERS_JSON = Path(\"../thirdparty/bliss/clusters.json\")\n",
    "CLIENTS_PKL   = Path(\"../benchmark/dataset/data/clients.pkl\")\n",
    "\n",
    "# Modulo base: clients.pkl ids are 0..12640 (inclusive) -> 12641 distinct base ids\n",
    "MOD_BASE = 12641\n",
    "\n",
    "# Reproducibility (also used for random tie-breaks and test client sampling)\n",
    "RNG_SEED = 42\n",
    "rng = random.Random(RNG_SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def list_files_only(folder: Path):\n",
    "    \"\"\"Return list of filenames (not paths) for files directly under `folder` (recursively).\"\"\"\n",
    "    # If you want only immediate children (non-recursive), replace rglob with iterdir.\n",
    "    return [p.name for p in folder.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "def count_chars(folder: Path, filenames):\n",
    "    \"\"\"Return dict filename -> number of characters read as UTF-8 (errors ignored).\"\"\"\n",
    "    out = {}\n",
    "    for name in filenames:\n",
    "        fpath = folder / name\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            out[name] = len(f.read())\n",
    "    return out\n",
    "\n",
    "def stable_descending_by_count_with_random_tiebreak(items, counts, rng_obj):\n",
    "    \"\"\"\n",
    "    items: list of filenames\n",
    "    counts: dict filename->count\n",
    "    rng_obj: random.Random instance with fixed seed\n",
    "    Returns a list of items sorted descending by count, ties broken randomly (but reproducibly).\n",
    "    \"\"\"\n",
    "    # Shuffle first (stable sort keeps this order for ties)\n",
    "    items_shuffled = items[:]\n",
    "    rng_obj.shuffle(items_shuffled)\n",
    "    items_shuffled.sort(key=lambda fn: counts[fn], reverse=True)\n",
    "    return items_shuffled\n",
    "\n",
    "def load_clusters_and_models(clusters_json: Path):\n",
    "    with open(clusters_json, \"r\") as f:\n",
    "        clusters = json.load(f)\n",
    "\n",
    "    # Build rank and model->cluster maps\n",
    "    cluster_rank = {}\n",
    "    model_to_cluster = {}\n",
    "    for c in clusters:\n",
    "        cid = int(c[\"id\"])\n",
    "        cluster_rank[cid] = float(c[\"rank\"])   # rank: lower is higher priority (rank=1 highest)\n",
    "        for m in c.get(\"models\", []):\n",
    "            model_to_cluster[m] = cid\n",
    "    return cluster_rank, model_to_cluster\n",
    "\n",
    "def load_id_to_model(clients_pkl: Path):\n",
    "    with open(clients_pkl, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    id_to_model = {}\n",
    "    if isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            id_to_model[int(v[\"id\"])] = v[\"model\"]\n",
    "    else:  # list/tuple\n",
    "        for v in obj:\n",
    "            id_to_model[int(v[\"id\"])] = v[\"model\"]\n",
    "    # minimal sanity: ensure we have 0..12640 (as per your spec)\n",
    "    # (We trust your guarantee; we don't fail if some are missing.)\n",
    "    return id_to_model\n",
    "\n",
    "def build_cluster_order(cluster_rank: dict):\n",
    "    \"\"\"Return list of cluster_ids ordered by ascending rank, tie-broken by cluster id.\"\"\"\n",
    "    return sorted(cluster_rank.keys(), key=lambda k: (cluster_rank[k], k))\n",
    "\n",
    "def map_new_client_to_cluster(new_client_id: int, id_to_model: dict, model_to_cluster: dict):\n",
    "    base_id = new_client_id % MOD_BASE\n",
    "    model = id_to_model[base_id]\n",
    "    return model_to_cluster[model]\n",
    "\n",
    "def assign_files_to_clients_by_rank(\n",
    "    folder: Path,\n",
    "    filenames_sorted_desc,                    # list of filenames sorted by char count desc (random tie-break)\n",
    "    new_client_ids,                           # list of new client ids (0..N-1)\n",
    "    id_to_model: dict,\n",
    "    model_to_cluster: dict,\n",
    "    cluster_order: list                       # list of cluster ids in ascending rank order\n",
    "):\n",
    "    \"\"\"\n",
    "    Assign files to clients so that all clients in the highest-rank cluster\n",
    "    get the longest files first, then the next cluster, etc.\n",
    "    Returns a list of (client_id, filename).\n",
    "    \"\"\"\n",
    "    # Group new clients by cluster (deterministic order within cluster: ascending id)\n",
    "    cluster_to_clients = defaultdict(list)\n",
    "    for cid in new_client_ids:\n",
    "        k = map_new_client_to_cluster(cid, id_to_model, model_to_cluster)\n",
    "        cluster_to_clients[k].append(cid)\n",
    "    for k in cluster_to_clients:\n",
    "        cluster_to_clients[k].sort()\n",
    "\n",
    "    # Build client order: all clients in cluster rank 1, then rank 2, ...\n",
    "    ordered_clients = []\n",
    "    for k in cluster_order:\n",
    "        ordered_clients.extend(cluster_to_clients.get(k, []))\n",
    "\n",
    "    if len(ordered_clients) != len(filenames_sorted_desc):\n",
    "        raise RuntimeError(\"Internal size mismatch between clients and files.\")\n",
    "\n",
    "    # Pair longest files to highest-rank cluster, etc.\n",
    "    pairs = list(zip(ordered_clients, filenames_sorted_desc))\n",
    "    # convert to rows\n",
    "    rows = [{\"client_id\": cid, \"sample_path\": fname, \"label_name\": -1, \"label_id\": -1}\n",
    "            for cid, fname in pairs]\n",
    "    return rows\n",
    "\n",
    "def write_csv(rows, out_csv: Path):\n",
    "    df = pd.DataFrame(rows, columns=[\"client_id\", \"sample_path\", \"label_name\", \"label_id\"])\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata\n",
    "# -------------------------\n",
    "cluster_rank, model_to_cluster = load_clusters_and_models(CLUSTERS_JSON)\n",
    "cluster_order = build_cluster_order(cluster_rank)\n",
    "id_to_model = load_id_to_model(CLIENTS_PKL)\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN: one client per file\n",
    "# -------------------------\n",
    "train_files = list_files_only(TRAIN_DIR)\n",
    "if not train_files:\n",
    "    raise RuntimeError(f\"No files found under {TRAIN_DIR}\")\n",
    "\n",
    "# count characters & sort (desc) with random tie-break\n",
    "train_counts = count_chars(TRAIN_DIR, train_files)\n",
    "train_files_sorted = stable_descending_by_count_with_random_tiebreak(train_files, train_counts, rng)\n",
    "\n",
    "# define train clients 0..N_train-1\n",
    "N_train = len(train_files_sorted)\n",
    "train_client_ids = list(range(N_train))\n",
    "\n",
    "# assign\n",
    "train_rows = assign_files_to_clients_by_rank(\n",
    "    folder=TRAIN_DIR,\n",
    "    filenames_sorted_desc=train_files_sorted,\n",
    "    new_client_ids=train_client_ids,\n",
    "    id_to_model=id_to_model,\n",
    "    model_to_cluster=model_to_cluster,\n",
    "    cluster_order=cluster_order\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# TEST: subset of train clients, one client per file\n",
    "# -------------------------\n",
    "test_files = list_files_only(TEST_DIR)\n",
    "if not test_files:\n",
    "    raise RuntimeError(f\"No files found under {TEST_DIR}\")\n",
    "\n",
    "# character counts + sort\n",
    "test_counts = count_chars(TEST_DIR, test_files)\n",
    "test_files_sorted = stable_descending_by_count_with_random_tiebreak(test_files, test_counts, rng)\n",
    "\n",
    "N_test = len(test_files_sorted)\n",
    "# sample a subset of train clients uniformly without replacement\n",
    "test_client_ids = sorted(rng.sample(train_client_ids, N_test))\n",
    "\n",
    "test_rows = assign_files_to_clients_by_rank(\n",
    "    folder=TEST_DIR,\n",
    "    filenames_sorted_desc=test_files_sorted,\n",
    "    new_client_ids=test_client_ids,\n",
    "    id_to_model=id_to_model,\n",
    "    model_to_cluster=model_to_cluster,\n",
    "    cluster_order=cluster_order\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Write CSVs (filename only in sample_path)\n",
    "# -------------------------\n",
    "write_csv(train_rows, OUTPUT_DIR / \"train.csv\")\n",
    "write_csv(test_rows,  OUTPUT_DIR / \"test.csv\")\n",
    "\n",
    "print(f\"Wrote: {OUTPUT_DIR / 'train.csv'}  (rows={len(train_rows)})\")\n",
    "print(f\"Wrote: {OUTPUT_DIR / 'test.csv'}   (rows={len(test_rows)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedscale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
